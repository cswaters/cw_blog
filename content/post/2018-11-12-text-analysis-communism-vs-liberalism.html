---
title: Text Analysis Communism vs Liberalism
author: Cory Waters
date: '2018-11-12'
slug: text-analysis-communism-vs-liberalism
categories:
  - R
tags:
  - text
---



<div id="text-analysis-of-political-tracts" class="section level2">
<h2>Text Analysis of Political Tracts</h2>
<p>The concepts and programming style in this post are inspired by the book <a href="https://www.tidytextmining.com">“Text Mining in R”</a>.</p>
<p>Most text mining tutorials you’ll come across on the web use modern (as in the last year to decade) sources. Way too many use twitter data. There might be value in sampling the “collective conscious” of the twitter-verse (I’m skeptical) but analysis of documents that have stood the test of time seems a more worthy endeavor.</p>
<p>Capitalism and Communism are the two most important political systems of the last hundred years. Each has its own canon of influential books. Defining the <em>most</em> influential isn’t the purpose of this post. For communism I’m selecting “The Communist Manifesto” by Karl Marx and Friedrich Engels. An argument can be made that “Das Kapital” is more influential, but it’s more technical and not suited for a general audience (IMO). For capitalism there’s a consensus that “An Inquiry into the Nature and Causes of the Wealth of Nations” by Adam Smith is the definitive work.</p>
<p>Both of these works are over 150 years old. The “Wealth of Nations” came out the same year as the American Revolution making it close to 250 years old. <a href="https://en.wikipedia.org/wiki/The_Communist_Manifesto">According to Wikipedia</a>, “The Communist Manifesto” was published in 1848 and was in the German language. Frederick Engels writes in the preface to a later edition that it was translated to English as early as 1850 but the “official” translation (the translation Engels approved of) <a href="https://www.marxists.org/archive/marx/works/1848/communist-manifesto/preface.htm">wasn’t published till 1888</a>.</p>
<p>Since both works are in the public domain they’re easily accessed via <a href="https://www.gutenberg.org">Project Gutenberg</a>. One of the great things about the <code>R</code> language is the number packages available. If you have an idea for a package chances are someone else has already created it and posted it online. That’s the case with the <code>gutenbergr</code> package. The documentation states that the <code>gutenberger</code> package lets users <a href="https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html">“Search and download public domain texts from Project Gutenberg”</a>.</p>
<p>Below we’ll use the <code>gutenbergr</code> to download both books and load them into data frames for further manipulation. We’re also going to use the <code>tidytext</code> and <code>wordcloud2</code> packages so you’ll need to download and install them both if they aren’t installed on your system.</p>
<p>Some notes about the process below.</p>
<ul>
<li><p>To download a book via project <code>gutenberger</code> we need to know the book’s id number. There’s a few ways to obtain this. The easiest is via the <code>gutenberg_metadata</code> data frame loaded with the package. <code>gutenberg_metadata</code> can be filtered by title, author, etc. We can also use Google and copy the id from the text’s webpage.</p></li>
<li><p>Some books are posted multiple times. Filtering on the title might result in a vector of different id numbers. Usually we would investigate to make sure we’re working with the correct edition (or book in some cases). In the example below we’re grabbing the first id and using it.</p></li>
</ul>
<pre class="r"><code>### Load Packages ###
library(tidyverse) # access to dplyr/tidyr/stringr functions
library(gutenbergr)
library(tidytext) # text analysis functions
library(wordcloud2) # visualize text
####################

### Download books ###
cm_id &lt;- gutenberg_metadata %&gt;% 
  filter(title == &#39;The Communist Manifesto&#39;) %&gt;% 
  pull(gutenberg_id)

won_id &lt;- gutenberg_metadata %&gt;% 
  filter(title == &#39;An Inquiry into the Nature and Causes of the Wealth of Nations&#39;) %&gt;% 
  pull(gutenberg_id)

# length(cm_id) # 2
cm_id &lt;- cm_id[1]
# length(won_id)

cm &lt;- gutenberg_download(cm_id[1])
won &lt;- gutenberg_download(won_id)</code></pre>
<div id="get-summary-of-books" class="section level3">
<h3>Get Summary of Books</h3>
<p>A quick <code>glimpse</code> of “The Communist Manifesto”.</p>
<pre class="r"><code>glimpse(cm)</code></pre>
<pre><code>## Observations: 1,496
## Variables: 2
## $ gutenberg_id &lt;int&gt; 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 6...
## $ text         &lt;chr&gt; &quot;Transcribed by Allen Lutins with assistance from...</code></pre>
<p>And a <code>glimpse</code> of “An Inquiry into the Nature and Causes of the Wealth of Nations.”</p>
<pre class="r"><code>glimpse(won)</code></pre>
<pre><code>## Observations: 35,202
## Variables: 2
## $ gutenberg_id &lt;int&gt; 3300, 3300, 3300, 3300, 3300, 3300, 3300, 3300, 3...
## $ text         &lt;chr&gt; &quot;AN INQUIRY INTO THE NATURE AND CAUSES OF THE WEA...</code></pre>
<pre class="r"><code>nrow(won) / nrow(cm)</code></pre>
<pre><code>## [1] 23.53075</code></pre>
<p>“Wealth of Nations” is a huge book compared to the “Communist Manifesto.” Almost 23 lines of Smith for every line of Marx &amp; Engels. For now we’ll ignore the size discrepancy.</p>
</div>
<div id="cleaning-up-the-books" class="section level3">
<h3>Cleaning up the books</h3>
<p>Often, books from Project Gutenberg contain blank lines and metadata in the first few lines.</p>
<pre class="r"><code>cm %&gt;% 
  head(10)</code></pre>
<pre><code>## # A tibble: 10 x 2
##    gutenberg_id text                                                      
##           &lt;int&gt; &lt;chr&gt;                                                     
##  1           61 Transcribed by Allen Lutins with assistance from Jim Tarz…
##  2           61 &quot;&quot;                                                        
##  3           61 &quot;&quot;                                                        
##  4           61 &quot;&quot;                                                        
##  5           61 &quot;&quot;                                                        
##  6           61 &quot;&quot;                                                        
##  7           61 &quot;&quot;                                                        
##  8           61 &quot;&quot;                                                        
##  9           61 &quot;&quot;                                                        
## 10           61 MANIFESTO OF THE COMMUNIST PARTY</code></pre>
<p>In this case it’s the first three lines. Another thing we’re going to want to do for later analysis is add the line number.</p>
<pre class="r"><code>cm &lt;- cm %&gt;% 
  filter(text != &#39;&#39;) %&gt;% 
  slice(-(1:3))

cm &lt;- cm %&gt;% 
  mutate(line = row_number(gutenberg_id))

cm %&gt;% 
  head(10)</code></pre>
<pre><code>## # A tibble: 10 x 3
##    gutenberg_id text                                                  line
##           &lt;int&gt; &lt;chr&gt;                                                &lt;int&gt;
##  1           61 A spectre is haunting Europe--the spectre of Commun…     1
##  2           61 All the Powers of old Europe have entered into a ho…     2
##  3           61 exorcise this spectre: Pope and Czar, Metternich an…     3
##  4           61 French Radicals and German police-spies.                 4
##  5           61 Where is the party in opposition that has not been …     5
##  6           61 Communistic by its opponents in power?  Where is th…     6
##  7           61 that has not hurled back the branding reproach of C…     7
##  8           61 against the more advanced opposition parties, as we…     8
##  9           61 its reactionary adversaries?                             9
## 10           61 Two things result from this fact.                       10</code></pre>
<p>We can chop off the first five lines of “Wealth of Nations”</p>
<pre class="r"><code>won &lt;- won %&gt;% 
  slice(-(1:5)) %&gt;% 
  filter(text != &#39;&#39;) %&gt;% 
  mutate(line = row_number(gutenberg_id))

won %&gt;% 
  head(10)</code></pre>
<pre><code>## # A tibble: 10 x 3
##    gutenberg_id text                                                  line
##           &lt;int&gt; &lt;chr&gt;                                                &lt;int&gt;
##  1         3300 INTRODUCTION AND PLAN OF THE WORK.                       1
##  2         3300 The annual labour of every nation is the fund which…     2
##  3         3300 it with all the necessaries and conveniencies of li…     3
##  4         3300 consumes, and which consist always either in the im…     4
##  5         3300 of that labour, or in what is purchased with that p…     5
##  6         3300 nations.                                                 6
##  7         3300 According, therefore, as this produce, or what is p…     7
##  8         3300 bears a greater or smaller proportion to the number…     8
##  9         3300 to consume it, the nation will be better or worse s…     9
## 10         3300 necessaries and conveniencies for which it has occa…    10</code></pre>
</div>
<div id="the-obligatory-word-cloud" class="section level3">
<h3>The obligatory word cloud</h3>
<p>No text analysis would be complete without a word cloud of the most common words. I’m not a huge fan of word clouds but I’m in the minority it seems. Let’s do two word clouds of the 100 most frequent words in each text (after stop words removed).</p>
<div id="removing-stop-words" class="section level4">
<h4>Removing “stop words”</h4>
<p>I can’t find a concise definition of stop words. Wikipedia says, “In computing, stop words are words which are filtered out before or after processing of natural language data.” Which words get filtered out is up to who makes the list. In short, stop words are the most common words “a”, “and”, “the”, etc. that convey no meaning. The <code>tidytext</code> package defaults to the “snowball” lexicon of stop words. It also supports any of the lexicons in the <code>stopwords</code> package.</p>
<pre class="r"><code>stopwords::stopwords_getsources()</code></pre>
<pre><code>## [1] &quot;snowball&quot;      &quot;stopwords-iso&quot; &quot;misc&quot;          &quot;smart&quot;</code></pre>
<p>Instead of using the <code>get_stopwords</code> function we’re going to use the preloaded <code>stop_words</code> data frame that loads with <code>tidytext</code>. To remove the stop words from a book we need to:</p>
<ol style="list-style-type: decimal">
<li>Tokenize the words in the work. This is easily done via the <code>unnest_tokens</code> function.</li>
<li><code>anti_join</code> the <code>stop_words</code> data frame to the tokenized book data frame. An <code>anti_join</code> removes the words from the first data frame that appear in both data frames.</li>
</ol>
<pre class="r"><code># a python lambda-function-esque way of writing a function in R
token_remove_stopwords &lt;- . %&gt;% 
  unnest_tokens(input = &#39;text&#39;,
                output = &#39;word&#39;) %&gt;% 
  anti_join(stop_words)

cm_token &lt;- cm %&gt;% 
  token_remove_stopwords()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>won_token &lt;- won %&gt;% 
  token_remove_stopwords()</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
</div>
</div>
<div id="group-by-most-frequent" class="section level3">
<h3>Group by most frequent</h3>
<p>Next we’ll group by <code>word</code>, determine the frequency, sort by most frequent and save the top 100 most frequent words as a separate data frame. The “Wealth of Nations” has an issue, Smith uses lots of numbers in the book. We don’t want 1’s or some other number taking up space in the word cloud. They don’t contain any information. So we’ll do a small hack to avoid that problem.</p>
<pre class="r"><code>top_100_token &lt;- . %&gt;% 
  count(word, sort = TRUE) %&gt;% 
  head(100)

top_100_cm &lt;- cm_token %&gt;% 
  top_100_token()

top_100_won &lt;- won_token %&gt;%
  mutate(nc = nchar(word)) %&gt;% # count the number of characters in each word
  filter(nc &gt; 2) %&gt;% # &quot;words&quot; of 3 containing three characters or more 
  top_100_token()</code></pre>
</div>
<div id="create-word-cloud" class="section level3">
<h3>Create word cloud</h3>
<p>Now we have everything we need to create our word clouds. We’ve already loaded the <code>wordcloud</code> package. The package has lots of options but we’ll keep it simple.</p>
<div id="the-communist-manifesto-word-cloud" class="section level4">
<h4>The Communist Manifesto Word Cloud</h4>
<pre class="r"><code>wordcloud2(top_100_cm, 
           color=&#39;#ff2400&#39;, 
           shape = &#39;star&#39;, size=.75)</code></pre>
<p><img src="/post/2018-11-12-text-analysis-communism-vs-liberalism_files/figure-html/cm_top100.png" /></p>
</div>
<div id="the-wealth-of-nations-word-cloud" class="section level4">
<h4>The Wealth of Nations Word Cloud</h4>
<pre class="r"><code>wordcloud2(top_100_won, 
           color=&#39;royalblue&#39;, 
           shape = &#39;diamond&#39;, size=.75)</code></pre>
<p><img src="/post/2018-11-12-text-analysis-communism-vs-liberalism_files/figure-html/won_top100.png" /></p>
</div>
</div>
<div id="n-gram-not-word-frequency" class="section level3">
<h3>N-Gram not word frequency</h3>
<p>While word tallies are useful, the combination of multiple words form more coherent ideas (at least visually). However, there’s one problem we can’t easily remove the stop words the way we did above. And n-grams (groups of n words) like “the price” don’t convey anymore information than “price.” To deal with this issue we need to do some wrangling.</p>
<p>The <code>make_ngrams</code> function does the following:</p>
<ol style="list-style-type: decimal">
<li><code>filter</code> out any blank lines.</li>
<li>split each line into a number of ngrams (n=2 in this case) via the <code>token=ngrams</code> parameter in the <code>unnest_tokens</code> function.</li>
<li><code>seperate</code> the ngram column (column with two words) into two unique columns, each with a word from the ngram.</li>
<li><code>filter</code> both words in the ngram, from the columns created in the step above, against the list of stop words.</li>
<li><code>count</code> the number of word combos created.</li>
<li>Get the number of character per word in each column, filter out words with less than 3 characters. This is an attempt to remove numbers from the ngrams.</li>
</ol>
<pre class="r"><code>make_ngrams &lt;- . %&gt;% 
  filter(text != &#39;&#39;) %&gt;% 
  unnest_tokens(input = &#39;text&#39;, 
                output = &#39;bigrams&#39;, 
                token = &#39;ngrams&#39;, 
                n=2) %&gt;% 
  separate(col = &#39;bigrams&#39;, 
           into = c(&#39;word1&#39;, &#39;word2&#39;), 
           sep = &#39; &#39;) %&gt;% 
  filter(!word1 %in% stop_words$word) %&gt;% 
  filter(!word2 %in% stop_words$word) %&gt;% 
  count(word1, word2, sort=TRUE) %&gt;% 
  mutate(nc1 = nchar(word1),
         nc2 = nchar(word2)) %&gt;% 
  filter(nc1 &gt; 2, nc2 &gt; 2)


won_ngrams &lt;- make_ngrams(won)
cm_ngrams &lt;- make_ngrams(cm)</code></pre>
</div>
<div id="bigram-word-cloud-for-wealth-of-nations" class="section level3">
<h3>Bigram word cloud for “Wealth of Nations”</h3>
<p>We recombine the two words in the bigram into one column, and take the 100 most common for the new word cloud.</p>
<pre class="r"><code>top_100_won_bigram &lt;- head(won_ngrams, 100) %&gt;%
  transmute(bigram = paste(word1, word2),
            n)
  
wordcloud2(top_100_won_bigram, 
           color=&#39;royalblue&#39;, 
           shape = &#39;diamond&#39;, size=.75)</code></pre>
<p><img src="/post/2018-11-12-text-analysis-communism-vs-liberalism_files/figure-html/won_bigram.png" /></p>
</div>
<div id="bigram-word-cloud-for-the-communist-manifesto" class="section level3">
<h3>Bigram word cloud for “The Communist Manifesto”</h3>
<p>Like above, we take the 100 most frequent bigrams and join them for the word cloud. Because of the small size of the “Communist Manifesto” the word cloud doesn’t have as many options and looks less dense.</p>
<pre class="r"><code>top_100_cm_bigram &lt;- head(cm_ngrams, 100) %&gt;%
  transmute(bigram = paste(word1, word2),
            n)

wordcloud2(top_100_cm_bigram, 
           color=&#39;#ff2400&#39;, 
           shape = &#39;star&#39;, size=.5)</code></pre>
<p><img src="/post/2018-11-12-text-analysis-communism-vs-liberalism_files/figure-html/cm_bigram.png" /></p>
</div>
</div>
<div id="sentiment-analysis" class="section level2">
<h2>Sentiment Analysis</h2>
<p>Wikipedia says, <a href="https://en.wikipedia.org/wiki/Sentiment_analysis">“Opinion mining (sometimes known as sentiment analysis or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.”</a> Usually when you see sentiment analysis in tutorials it’s done on tweets. This makes sense, the algorithm is trying to gauge how positive or negative a tweet, email, chat, etc is. From what I’ve read some stock prediction models use sentiment analysis applied to news and social media to assess public opinion about a company or stock.</p>
<p>In our example we’re going to do sentiment analysis on both books of political theory we used above. This is similar to the examples in the “Text mining in R” book which uses Jane Austen novels.</p>
<p>Using tidy tools it’s absurdly easy to do sentiment analysis.</p>
<p>Let’s breakdown the code below:</p>
<ol style="list-style-type: decimal">
<li>We get the “Bing” sentiment lexicon via the <code>get_sentiments</code> function and assign it to <code>bing</code></li>
<li>Then join the <code>cm_token</code> data frame (which is a data frame of the word counts in the “Communist Manifesto”) with the <code>bing</code> data frame. This appends the sentiment score to the words in the book.</li>
<li>Think of the <code>index</code> as a faux chapter we need to group the book along a evenly spaced vector to aggregate the sentiment score.</li>
<li>Group the data frame by the sentiment and the index, i.e. number positive words in index <em>n</em> and number of negative words in index <em>n</em>.</li>
<li>Go from long to wide for each index, creating a row per index number with a column for the number of negative and positive words.</li>
<li>Calculate the sentiment for each index by subtracting the number of positive terms from negative terms. The last calculation is to divide the sentiment number by the number of lines per index. This is so we can standardize the scores vs the much larger “Wealth of Nations.”</li>
</ol>
<pre class="r"><code>bing &lt;- get_sentiments(&quot;bing&quot;)

cm_sentiment &lt;- cm_token %&gt;%
  inner_join(bing) %&gt;%
  count(index = line %/% 20, sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment = (positive - negative)/20)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p>Now we can visualize the sentiment progression of the book.</p>
<pre class="r"><code>cm_sentiment %&gt;% 
  ggplot(aes(index, sentiment)) +
  geom_col(fill=&#39;#FF2400&#39;) +
  theme_minimal()</code></pre>
<p><img src="/post/2018-11-12-text-analysis-communism-vs-liberalism_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Based on the <code>Bing</code> lexicon “The Communist Manifesto” is a rather “negative” book. There’s a few of issues here.</p>
<ol style="list-style-type: decimal">
<li>The book was originally published in German. Does the translation process contribute to the “sentiment” of a work? I’m no expert but I doubt that it would be a very positive book in the original German.</li>
<li>The number of lines per index (similar to a bin in a histogram) can produce artifacts that have more to do with the cut point than the content.</li>
<li>Does the way one views Communism and Socialism impact the sentiment. A common term in “The Communist Manifesto” is “class struggle.” Clearly this has negative sentiment according to the lexicon.</li>
</ol>
<pre class="r"><code>bing %&gt;% 
  filter(word == &#39;struggle&#39;)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   word     sentiment
##   &lt;chr&gt;    &lt;chr&gt;    
## 1 struggle negative</code></pre>
<p>But if someone views class struggle as the process that happens on the path to communism it doesn’t have the same negative undertones.</p>
<p>Nonetheless, “The Communist Manifesto” is a work with a negative sentiment. Let’s apply the same analysis to the “Wealth of Nations.” The only difference is the number of lines per index. This makes sense since the “Wealth of Nations” is a much larger work. If we want the length of the sentiment analysis to be roughly the same size across graphs there needs to be many more lines per index for the “Wealth of Nations.” Earlier we measured that the “Wealth of Nations” contained approx 23 lines per every line in “The Communist Manifesto.” The lines per index above is set at 20. Setting the “Wealth of Nations” at 480 lines per index is the optimal size for a comparison.</p>
<pre class="r"><code>won_sentiment &lt;- won_token %&gt;%
  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
  count(index = line %/% 480, sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment = (positive - negative)/480)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p>Based on the analysis Smith’s work is very positive. I personally believe that these results are representative of the philosophical outlook of each system.</p>
<p>To wrap it up with cliche, communism views the world as a big pie and we need to redistribute the pieces. Classical liberalism views the world as an ever expanding pie, and one person’s gain doesn’t come at the expense of others. Trade makes us all richer.</p>
<pre class="r"><code>won_sentiment %&gt;% 
  ggplot(aes(index, sentiment)) +
  geom_col(fill=&#39;#012169&#39;) +
  theme_minimal()</code></pre>
<p><img src="/post/2018-11-12-text-analysis-communism-vs-liberalism_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
